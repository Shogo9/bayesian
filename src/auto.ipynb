{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import json\n",
    "import os\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "from tqdm.notebook import tqdm\n",
    "import optuna\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import numpy as np\n",
    "import joblib\n",
    "import sys\n",
    "import io\n",
    "from torchsummary import summary\n",
    "from torchviz import make_dot\n",
    "import random\n",
    "from colorama import Fore, Style\n",
    "import matplotlib.pyplot as plt\n",
    "import shutil\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CPUスレッド数を設定\n",
    "torch.set_num_threads(3)\n",
    "\n",
    "\"\"\"parameter\"\"\"\n",
    "# ランダムシード\n",
    "random_seed = 42\n",
    "# バリデーションの割合\n",
    "val_split = 0.2\n",
    "# inputdata数, outputdata数\n",
    "input_dim = 3\n",
    "output_dim = 2\n",
    "# early stopping\n",
    "patience = 10000  # 検証損失が改善しない許容エポック数\n",
    "delta = 1e-4   # 検証損失の改善として認める最小値\n",
    "# トライアル数\n",
    "n_trials = 100\n",
    "# 推論データ\n",
    "test_x = 3 # x軸\n",
    "test_y = 3 # y軸\n",
    "test_z = 3 # z軸\n",
    "# 学習率\n",
    "lr_min = 1e-6\n",
    "lr_max = 1e-1\n",
    "# バッチサイズ\n",
    "batch_size_min = 4\n",
    "batch_size_max = 64\n",
    "# エポック数\n",
    "epochs_min = 10\n",
    "epochs_max = 1000000\n",
    "# モデルの層数\n",
    "n_layer_min = 1\n",
    "n_layer_max = 20\n",
    "# 各層のユニット数\n",
    "model_unit_min = 4\n",
    "model_unit_max = 8192\n",
    "# ドロップアウト\n",
    "drop_out = 0.4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_json_data(input_path, output_path, scaler_dir=\"../data/\"):\n",
    "    try:\n",
    "        # JSONファイルを読み込み\n",
    "        with open(input_path, \"r\") as file:\n",
    "            data = json.load(file)\n",
    "        \n",
    "        # 必要なキーが存在するか確認\n",
    "        if \"inputs\" not in data or \"outputs\" not in data:\n",
    "            print(\"Error: 'inputs' or 'outputs' key not found in the JSON file.\")\n",
    "            return\n",
    "        \n",
    "        # 入力データと出力データをNumPy配列に変換\n",
    "        input_data = np.array(data[\"inputs\"])\n",
    "        output_data = np.array(data[\"outputs\"])\n",
    "    \n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: {input_path} not found.\")\n",
    "        return\n",
    "    except json.JSONDecodeError:\n",
    "        print(f\"Error: Failed to parse JSON file: {input_path}.\")\n",
    "        return\n",
    "    except Exception as e:\n",
    "        print(f\"Unexpected error while reading JSON file: {e}\")\n",
    "        return\n",
    "\n",
    "    # スケーラーをインスタンス化\n",
    "    input_scaler = MinMaxScaler()\n",
    "    output_scaler = MinMaxScaler()\n",
    "\n",
    "    try:\n",
    "        # 入力と出力を正規化\n",
    "        inputs_normalized = input_scaler.fit_transform(input_data)\n",
    "        outputs_normalized = output_scaler.fit_transform(output_data)\n",
    "\n",
    "        # 正規化データを辞書にまとめる\n",
    "        normalized_data = {\n",
    "            \"inputs\": inputs_normalized.tolist(),\n",
    "            \"outputs\": outputs_normalized.tolist()\n",
    "        }\n",
    "\n",
    "        # 正規化データをファイルに保存\n",
    "        with open(output_path, \"w\") as output_file:\n",
    "            json.dump(normalized_data, output_file)\n",
    "        \n",
    "        # スケーラーを保存するディレクトリを作成（存在しない場合）\n",
    "        os.makedirs(scaler_dir, exist_ok=True)\n",
    "        \n",
    "        # スケーラーを保存\n",
    "        input_scaler_path = os.path.join(scaler_dir, \"input_scaler.pkl\")\n",
    "        output_scaler_path = os.path.join(scaler_dir, \"output_scaler.pkl\")\n",
    "        with open(input_scaler_path, \"wb\") as input_scaler_file:\n",
    "            joblib.dump(input_scaler, input_scaler_file)\n",
    "        with open(output_scaler_path, \"wb\") as output_scaler_file:\n",
    "            joblib.dump(output_scaler, output_scaler_file)\n",
    "\n",
    "        print(\"Normalization and scaler saving completed successfully.\")\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error during normalization or saving process: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, json_path):\n",
    "        with open(json_path, 'r') as f:\n",
    "            data = json.load(f)\n",
    "        self.inputs = data[\"inputs\"]\n",
    "        self.outputs = data[\"outputs\"]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.inputs)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        x = torch.tensor(self.inputs[idx], dtype=torch.float32)\n",
    "        y = torch.tensor(self.outputs[idx], dtype=torch.float32)\n",
    "        return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dataloaders(json_path, batch_size=16, val_split=val_split, random_seed=random_seed):\n",
    "    dataset = CustomDataset(json_path)\n",
    "    \n",
    "    # 乱数シードを設定\n",
    "    torch.manual_seed(random_seed)\n",
    "    random.seed(random_seed)\n",
    "    \n",
    "    # 訓練データと検証データのサイズ計算\n",
    "    dataset_size = len(dataset)\n",
    "    val_size = int(dataset_size * val_split)\n",
    "    train_size = dataset_size - val_size\n",
    "    \n",
    "    # データセットを分割\n",
    "    train_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n",
    "    \n",
    "    # DataLoaderの作成\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "    \n",
    "    return train_loader, val_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NetworkModel(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim, trial, n_layers_min=n_layer_min, n_layers_max=n_layer_max, model_unit_min=model_unit_min, model_unit_max=model_unit_max, drop_out=drop_out):\n",
    "        super(NetworkModel, self).__init__()\n",
    "\n",
    "        # 層数をOptunaで指定\n",
    "        n_layers = trial.suggest_int(\"n_layers\", n_layers_min, n_layers_max)\n",
    "\n",
    "        # 各層のユニット数をOptunaで指定\n",
    "        layers = []\n",
    "        in_features = input_dim\n",
    "        for i in range(n_layers):\n",
    "            out_features = trial.suggest_int(f\"n_units_l{i}\", model_unit_min, model_unit_max)  # 各層のユニット数\n",
    "            layers.append(nn.Linear(in_features, out_features))\n",
    "            layers.append(nn.ReLU())\n",
    "            if trial.suggest_float(f\"dropout_l{i}\", 0.0, drop_out) > 0.0:\n",
    "                layers.append(nn.Dropout(trial.suggest_float(f\"dropout_l{i}\", 0.0, drop_out)))\n",
    "            in_features = out_features\n",
    "        \n",
    "        # 出力層\n",
    "        layers.append(nn.Linear(in_features, output_dim))\n",
    "        \n",
    "        # nn.Sequentialにまとめる\n",
    "        self.network = nn.Sequential(*layers)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.network(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# トライアル終了後のコールバックで保存\n",
    "def save_trial(trial, model, save_dir=\"../results/model\"):\n",
    "    trial_params = trial.params\n",
    "    trial_number = trial.number + 1\n",
    "    \n",
    "    # 保存するディレクトリを作成\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "    # ハイパーパラメータをJSONファイルに保存\n",
    "    with open(f\"{save_dir}/trial_{trial_number}_params.json\", \"w\") as f:\n",
    "        json.dump(trial_params, f, indent=4)\n",
    "\n",
    "    # モデルの状態を保存\n",
    "    torch.save(model.state_dict(), f\"{save_dir}/trial_{trial_number}_model.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_and_prepare_directories():\n",
    "    # クリーンアップ対象のディレクトリを指定\n",
    "    directories_to_clean = [\n",
    "        \"../data/summary\",\n",
    "        \"../images/model_graph\",\n",
    "        \"../images/graph_training_results\",\n",
    "        \"../images/graph_val_results\",\n",
    "        \"../results\"\n",
    "    ]\n",
    "    \n",
    "    # 各ディレクトリ内の既存ファイルを削除\n",
    "    for dir_path in directories_to_clean:\n",
    "        if os.path.exists(dir_path):\n",
    "            shutil.rmtree(dir_path)  # ディレクトリとその中身を削除\n",
    "        os.makedirs(dir_path, exist_ok=True)  # 再作成\n",
    "\n",
    "    print(\"Directories cleaned and prepared.\")\n",
    "\n",
    "# トレーニング開始前にディレクトリをクリーンアップ\n",
    "clean_and_prepare_directories()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# モデルを作成する関数\n",
    "def create_model(trial, input_dim, output_dim):\n",
    "    return NetworkModel(input_dim, output_dim, trial)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(trial, input_dim=input_dim, output_dim=output_dim, patience=patience, delta=delta, lr_min=lr_min, lr_max=lr_max, batch_size_min=batch_size_min, batch_size_max=batch_size_max, epochs_min=epochs_min, epochs_max=epochs_max):\n",
    "    # モデルの作成\n",
    "    input_dim = input_dim\n",
    "    output_dim = output_dim\n",
    "    model = create_model(trial, input_dim, output_dim)\n",
    "    \n",
    "    # モデルを trial.user_attrs に保存\n",
    "    trial.set_user_attr(\"model\", model)\n",
    "    \n",
    "        # トライアルごとに保存\n",
    "    trial_number = trial.number + 1  # トライアル番号\n",
    "    \n",
    "    # Early Stoppingのパラメータ\n",
    "    patience = patience  # 検証損失が改善しない許容エポック数\n",
    "    delta = delta   # 検証損失の改善として認める最小値\n",
    "    early_stop_counter = 0  # 改善しないエポック数をカウント\n",
    "    \n",
    "    # モデルサマリーの保存\n",
    "    os.makedirs(\"../data/summary\", exist_ok=True)\n",
    "    stdout_backup = sys.stdout  # 標準出力のバックアップ\n",
    "    sys.stdout = io.StringIO()  # 標準出力をキャプチャ\n",
    "    summary(model, (1, input_dim))  # モデルのサマリーを取得 (バッチサイズ1の例)\n",
    "    summary_str = sys.stdout.getvalue()  # サマリー内容を取得\n",
    "    sys.stdout = stdout_backup  # 標準出力を元に戻す\n",
    "    with open(f\"../data/summary/model_summary_trial_{trial_number}.txt\", \"w\") as f:\n",
    "        f.write(summary_str)\n",
    "\n",
    "    # 計算グラフを保存\n",
    "    os.makedirs(\"../images/model_graph/\", exist_ok=True)\n",
    "    x = torch.randn(1, input_dim)  # ダミー入力データを作成\n",
    "    y = model(x)  # ダミー入力を通して出力を取得\n",
    "    dot = make_dot(y, params=dict(model.named_parameters()))  # 計算グラフを作成\n",
    "    dot.format = \"png\"  # PNG形式で保存\n",
    "    dot.render(f\"../images/model_graph/model_graph_trial_{trial_number}\")  # ファイル名にトライアル番号を付与\n",
    "    \n",
    "    \n",
    "    # Optunaのトライアルからハイパーパラメータを取得\n",
    "    lr = trial.suggest_float(\"lr\", lr_min, lr_max)\n",
    "    batch_size = trial.suggest_int(\"batch_size\", batch_size_min, batch_size_max)\n",
    "    epochs = trial.suggest_int(\"epochs\", epochs_min, epochs_max)\n",
    "    optimizer_name = trial.suggest_categorical(\n",
    "    \"optimizer\", [\"Adam\", \"SGD\", \"RMSprop\", \"AdamW\", \"Adagrad\", \"Adadelta\", \"Adamax\"]\n",
    ")\n",
    "    loss_function_name = trial.suggest_categorical(\n",
    "    \"loss_function\", [\"MSELoss\", \"L1Loss\", \"SmoothL1Loss\", \"HuberLoss\", \"BCEWithLogitsLoss\", \"CrossEntropyLoss\"]\n",
    ")\n",
    "    \n",
    "    # Optimizer を選択\n",
    "    if optimizer_name == \"Adam\":\n",
    "        optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "    elif optimizer_name == \"SGD\":\n",
    "        momentum = trial.suggest_float(\"momentum\", 0.0, 0.9)  # SGD の場合のみ momentum を提案\n",
    "        optimizer = optim.SGD(model.parameters(), lr=lr, momentum=momentum)\n",
    "    elif optimizer_name == \"RMSprop\":\n",
    "        optimizer = optim.RMSprop(model.parameters(), lr=lr)\n",
    "    elif optimizer_name == \"AdamW\":\n",
    "        weight_decay = trial.suggest_float(\"weight_decay\", 1e-6, 1e-2)  # AdamW用のweight decay\n",
    "        optimizer = optim.AdamW(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "    elif optimizer_name == \"Adagrad\":\n",
    "        optimizer = optim.Adagrad(model.parameters(), lr=lr)\n",
    "    elif optimizer_name == \"Adadelta\":\n",
    "        optimizer = optim.Adadelta(model.parameters(), lr=lr)\n",
    "    elif optimizer_name == \"Adamax\":\n",
    "        optimizer = optim.Adamax(model.parameters(), lr=lr)\n",
    "            \n",
    "    # 損失関数を選択\n",
    "    if loss_function_name == \"MSELoss\":\n",
    "        criterion = nn.MSELoss()\n",
    "    elif loss_function_name == \"L1Loss\":\n",
    "        criterion = nn.L1Loss()\n",
    "    elif loss_function_name == \"SmoothL1Loss\":\n",
    "        beta = trial.suggest_float(\"beta\", 0.1, 1.0)  # SmoothL1Loss 用の beta を提案\n",
    "        criterion = nn.SmoothL1Loss(beta=beta)\n",
    "    elif loss_function_name == \"HuberLoss\":\n",
    "        delta = trial.suggest_float(\"delta\", 1.0, 10.0)  # HuberLoss用のdelta\n",
    "        criterion = nn.HuberLoss(delta=delta)\n",
    "    elif loss_function_name == \"BCEWithLogitsLoss\":\n",
    "        criterion = nn.BCEWithLogitsLoss()\n",
    "    elif loss_function_name == \"CrossEntropyLoss\":\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "    \n",
    "    # データローダーの取得\n",
    "    train_loader, val_loader = get_dataloaders('../data/normalized_data.json', batch_size=batch_size)\n",
    "    \n",
    "    train_losses = []  # 訓練損失を記録\n",
    "    val_losses = [] #検証損失の記録\n",
    "    val_accuracies = [] #バリデーション精度の記録\n",
    "    \n",
    "    best_loss = float(\"inf\")\n",
    "    best_model = None\n",
    "    \n",
    "    # トレーニングの進行状況をプログレスバーで表示\n",
    "    progress_bar = tqdm(\n",
    "        range(epochs),\n",
    "        desc=f\"Trial {trial_number} Progress\",  # トライアル番号付きの進行状況\n",
    "        unit=\"epoch\",\n",
    "        bar_format=\"{l_bar}{bar:40}| {n_fmt}/{total_fmt} epochs [{elapsed}<{remaining}] - {postfix}\"\n",
    "        )\n",
    "    \n",
    "    # エポックごとの訓練ループ\n",
    "    for epoch in progress_bar:\n",
    "        # 訓練フェーズ\n",
    "        model.train()  # モデルを訓練モードに設定\n",
    "        total_train_loss = 0  # 訓練損失の合計\n",
    "\n",
    "        for x, y in train_loader:\n",
    "            optimizer.zero_grad()  # 勾配をゼロにリセット\n",
    "            y_pred = model(x)  # モデルで予測\n",
    "            loss = criterion(y_pred, y)  # 損失を計算\n",
    "            loss.backward()  # 逆伝播\n",
    "            optimizer.step()  # パラメータを更新\n",
    "            total_train_loss += loss.item()  # 訓練損失を累積\n",
    "\n",
    "        avg_train_loss = total_train_loss / len(train_loader)  # 平均訓練損失\n",
    "        train_losses.append(avg_train_loss)  # 訓練損失を記録\n",
    "\n",
    "        # 検証フェーズ\n",
    "        model.eval()  # モデルを評価モードに設定\n",
    "        total_val_loss = 0  # 検証損失の合計\n",
    "        correct_predictions = 0  # 正しい予測の数\n",
    "        total_samples = 0  # サンプル数\n",
    "\n",
    "        with torch.no_grad():  # 検証時は勾配計算を行わない\n",
    "            for x, y in val_loader:\n",
    "                y_pred = model(x)  # モデルで予測\n",
    "                loss = criterion(y_pred, y)  # 損失を計算\n",
    "                total_val_loss += loss.item()  # 検証損失を累積\n",
    "\n",
    "                # 精度を計算\n",
    "                _, predicted = torch.max(y_pred, 1)  # 予測結果\n",
    "                _, labels = torch.max(y, 1)  # 正解ラベル\n",
    "                correct_predictions += (predicted == labels).sum().item()  # 正しい予測の数をカウント\n",
    "                total_samples += labels.size(0)  # サンプル数をカウント\n",
    "\n",
    "        avg_val_loss = total_val_loss / len(val_loader)  # 平均検証損失\n",
    "        val_losses.append(avg_val_loss)  # 検証損失を記録\n",
    "\n",
    "        accuracy = 100 * correct_predictions / total_samples  # 精度の計算\n",
    "        val_accuracies.append(accuracy)  # 精度を記録\n",
    "\n",
    "        # 最良モデルを保存\n",
    "        if avg_val_loss < best_loss - delta:\n",
    "            best_loss = avg_val_loss\n",
    "            best_model = model.state_dict()\n",
    "            early_stop_counter = 0  # 改善があればカウンターをリセット\n",
    "\n",
    "        else:\n",
    "            early_stop_counter += 1  # 改善しない場合にカウンターを増加\n",
    "            \n",
    "         # プログレスバーに詳細な情報を表示\n",
    "        progress_bar.set_postfix({\n",
    "            \"TrainLoss\": f\"{avg_train_loss:.4f}\",\n",
    "            \"ValLoss\": f\"{avg_val_loss:.4f}\",\n",
    "            \"ValAcc\": f\"{accuracy:.2f}%\",\n",
    "            \"ES\": f\"{early_stop_counter}/{patience}\",  # Early stoppingのカウントと許容値\n",
    "            \"LR\": f\"{lr:.2e}\",  # 学習率を科学記号形式で表示\n",
    "            \"Batch\": batch_size,\n",
    "            \"Opt\": optimizer.__class__.__name__,\n",
    "            \"LossFn\": loss_function_name\n",
    "        })\n",
    "\n",
    "        # Optunaの進捗を報告\n",
    "        trial.report(avg_val_loss, epoch)\n",
    "        \n",
    "        # Early Stoppingの判定\n",
    "        if early_stop_counter >= patience:\n",
    "            print(f\"Early stopping at epoch {epoch + 1}\")\n",
    "            break\n",
    "\n",
    "        # プルーニングが必要ならば中断\n",
    "        \"\"\"if trial.should_prune():\n",
    "            raise optuna.exceptions.TrialPruned()\"\"\"\n",
    "        \n",
    "        \n",
    "        \n",
    "    # トライアルごとに学習曲線とバリデーション精度をプロットして保存\n",
    "    os.makedirs('../images/graph_training_results', exist_ok=True)  # images/graphディレクトリがない場合は作成\n",
    "    os.makedirs('../images/graph_val_results', exist_ok=True)\n",
    "\n",
    "    # 学習曲線をプロット\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(train_losses, label=\"Train Loss\")\n",
    "    plt.plot(val_losses, label=\"Validation Loss\")\n",
    "    plt.xlabel(\"Epochs\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.title(f\"Training and Validation Loss (Trial {trial_number})\")\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.savefig(f'../images/graph_training_results/training_validation_loss_trial_{trial_number}.png')  # トライアルごとに保存\n",
    "    plt.close()\n",
    "\n",
    "    # バリデーション精度のグラフをプロット\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(val_accuracies, label=\"Validation Accuracy\")\n",
    "    plt.xlabel(\"Epochs\")\n",
    "    plt.ylabel(\"Accuracy (%)\")\n",
    "    plt.title(f\"Validation Accuracy (Trial {trial_number})\")\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.savefig(f'../images/graph_val_results/validation_accuracy_trial_{trial_number}.png')  # トライアルごとに保存\n",
    "    plt.close()\n",
    "\n",
    "\n",
    "    if not os.path.exists('../results'):\n",
    "        os.makedirs('../results')\n",
    "        \n",
    "    # トライアルごとに最良モデルを保存\n",
    "    torch.save(best_model, f'../results/best_model_trial_{trial_number}.pth')  # トライアルごとに保存\n",
    "\n",
    "    # 最良モデル（全トライアル）を保存\n",
    "    if best_model is not None:\n",
    "        # 最初のトライアルであれば最良モデルを保存、次のトライアルで更新\n",
    "        if trial.number == 0 or avg_val_loss < best_loss:\n",
    "            torch.save(best_model, '../results/best_model.pth')\n",
    "\n",
    "    return best_loss # 最良の検証損失を返す"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def perform_bayesian_optimization(n_trials):\n",
    "    \"\"\"Optunaによるベイズ最適化の実行\"\"\"\n",
    "    study = optuna.create_study(direction='minimize')\n",
    "    \n",
    "    # プログレスバーの設定\n",
    "    progress_bar = tqdm(\n",
    "        total=n_trials,\n",
    "        desc=f\"{Fore.MAGENTA}Bayesian Optimization Progress{Style.RESET_ALL}\",\n",
    "        unit=\"trial\",\n",
    "        bar_format=\"{l_bar}{bar:30}| {n_fmt}/{total_fmt} [{elapsed}<{remaining}] - {desc}\"\n",
    "    )\n",
    "\n",
    "    def callback(study, trial):\n",
    "        # プログレスバーの更新\n",
    "        progress_bar.set_description(\n",
    "            f\"{Fore.BLUE}Trial {trial.number+1}{Style.RESET_ALL} | Best Loss: {Fore.RED}{study.best_value:.6f}{Style.RESET_ALL}\"\n",
    "        )\n",
    "        progress_bar.update(1)\n",
    "\n",
    "        # ベストハイパーパラメータと損失をリアルタイム表示\n",
    "        print(f\"\\n{Fore.YELLOW}Current Best Hyperparameters:{Style.RESET_ALL} {study.best_params}\")\n",
    "        print(f\"{Fore.YELLOW}Current Best Loss:{Style.RESET_ALL} {study.best_value:.6f}\")\n",
    "\n",
    "    # ベイズ最適化の実行\n",
    "    study.optimize(\n",
    "        lambda trial: train_model(trial),  # 損失値を返す関数\n",
    "        n_trials=n_trials,\n",
    "        callbacks=[callback, lambda study, trial: save_trial(trial, trial.user_attrs[\"model\"])]\n",
    "    )\n",
    "\n",
    "    # プログレスバーを閉じる\n",
    "    progress_bar.close()\n",
    "\n",
    "    # 最終結果の表示\n",
    "    best_trial = study.best_trial\n",
    "    print(f\"{Fore.GREEN}Best Trial Number:{Style.RESET_ALL} {best_trial.number+1}\")\n",
    "    print(f\"{Fore.GREEN}Best Hyperparameters:{Style.RESET_ALL} {best_trial.params}\")\n",
    "    print(f\"{Fore.GREEN}Best Loss:{Style.RESET_ALL} {best_trial.value}\")\n",
    "\n",
    "    return best_trial.params, best_trial.number + 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# JSONデータの正規化\n",
    "normalize_json_data('../data/data.json', '../data/normalized_data.json')\n",
    "\n",
    "# Optunaによるベイズ最適化の実行\n",
    "best_params, best_number = perform_bayesian_optimization(n_trials=n_trials)\n",
    "print(\"Optimization completed. Best parameters:\", best_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model_from_trial(input_dim=input_dim, output_dim=output_dim, params_path=f\"../results/model/trial_{best_number}_params.json\", model_path=f\"../results/model/trial_{best_number}_model.pth\"):\n",
    "    # ハイパーパラメータを読み込む\n",
    "    with open(params_path, \"r\") as f:\n",
    "        params = json.load(f)\n",
    "    \n",
    "    # ダミーのトライアルオブジェクトを作成\n",
    "    class DummyTrial:\n",
    "        def __init__(self, params):\n",
    "            self.params = params\n",
    "        \n",
    "        def suggest_int(self, name, low, high):\n",
    "            return self.params[name]\n",
    "        \n",
    "        def suggest_float(self, name, low, high):\n",
    "            return self.params[name]\n",
    "    \n",
    "    dummy_trial = DummyTrial(params)\n",
    "    \n",
    "    # モデルを構築\n",
    "    model = NetworkModel(input_dim, output_dim, dummy_trial)\n",
    "    model.load_state_dict(torch.load(model_path))\n",
    "    model.eval()\n",
    "    \n",
    "    input_scaler = joblib.load('../data/input_scaler.pkl')\n",
    "    output_scaler = joblib.load('../data/output_scaler.pkl')\n",
    "    \n",
    "    return model, input_scaler, output_scaler\n",
    "\n",
    "model, input_scaler, output_scaler = load_model_from_trial()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " # 推論用関数\n",
    "def generate_output(input_data, model=model, input_scaler=input_scaler, output_scaler=output_scaler):\n",
    "    input_data_scaled = input_scaler.transform([input_data])\n",
    "    input_tensor = torch.tensor(input_data_scaled, dtype=torch.float32)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        output_scaled = model(input_tensor)\n",
    "        output = output_scaler.inverse_transform(output_scaled.numpy())\n",
    "\n",
    "    return output[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 推論\n",
    "test_input = [test_x, test_y, test_z]\n",
    "output = generate_output(test_input)\n",
    "print(f\"Input data: {test_input}\")\n",
    "print(f\"Generated Output: {output}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "# 最大値を初期化\n",
    "max_input = [0, 0, 0]\n",
    "max_output = [-1e+1000000, -1e+1000000]\n",
    "\n",
    "# 出力を計算する関数\n",
    "def process_point(i, j, k, model, input_scaler, output_scaler):\n",
    "    inference_input = [i, j, k]\n",
    "    out_put = generate_output(inference_input, model, input_scaler, output_scaler)\n",
    "    return out_put[1], [inference_input[0], inference_input[1], inference_input[2]], out_put\n",
    "\n",
    "# tqdmで進行状況を表示\n",
    "with ThreadPoolExecutor() as executor:\n",
    "    futures = []\n",
    "    for i in tqdm(range(500), desc=\"データ収集\"):\n",
    "        for j in range(500):\n",
    "            for k in range(500):\n",
    "                # 各スレッドに処理を割り当て\n",
    "                futures.append(executor.submit(process_point, i, j, k, model, input_scaler, output_scaler))\n",
    "\n",
    "    # 結果を処理\n",
    "    for future in tqdm(as_completed(futures), desc=\"結果処理中\", total=len(futures)):\n",
    "        out_put_value, input_value, output_value = future.result()\n",
    "        if max_output[1] < out_put_value:\n",
    "            max_output = output_value\n",
    "            max_input = input_value\n",
    "\n",
    "print(\"Max Input:\", max_input)\n",
    "print(\"Max Output:\", max_output)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
