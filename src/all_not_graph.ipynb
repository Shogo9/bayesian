{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.autograd import Variable\n",
    "import json\n",
    "import os\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tqdm.notebook import tqdm\n",
    "import optuna\n",
    "from optuna.exceptions import TrialPruned\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import numpy as np\n",
    "import joblib\n",
    "import sys\n",
    "import io\n",
    "from torchsummary import summary\n",
    "from torchviz import make_dot\n",
    "import random\n",
    "from colorama import Fore, Style\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "import shutil  # ディレクトリの削除に使用\n",
    "import concurrent.futures\n",
    "from multiprocessing import Manager\n",
    "from concurrent.futures import ProcessPoolExecutor\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CPUスレッド数を設定\n",
    "torch.set_num_threads(4)\n",
    "\n",
    "\"\"\"parameter\"\"\"\n",
    "# ランダムシード\n",
    "random_seed = 42\n",
    "# バリデーションの割合\n",
    "val_split = 0.2\n",
    "# inputdata数, outputdata数\n",
    "input_dim = 3\n",
    "output_dim = 2\n",
    "# early stopping\n",
    "patience = 50  # 検証損失が改善しない許容エポック数\n",
    "delta = 1e-4   # 検証損失の改善として認める最小値\n",
    "# トライアル数\n",
    "n_trials = 3\n",
    "# 推論データ\n",
    "test_x = 100 # x軸\n",
    "test_y = 100 # y軸\n",
    "test_z = 100 # z軸\n",
    "# 学習率\n",
    "lr_min = 1e-6\n",
    "lr_max = 1e-1\n",
    "# バッチサイズ\n",
    "batch_size_min = 4\n",
    "batch_size_max = 64\n",
    "# エポック数\n",
    "epochs_min = 10\n",
    "epochs_max = 10000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_json_data(input_path, output_path, scaler_dir=\"../data/\"):\n",
    "    try:\n",
    "        # JSONファイルを読み込み\n",
    "        with open(input_path, \"r\") as file:\n",
    "            data = json.load(file)\n",
    "        \n",
    "        # 必要なキーが存在するか確認\n",
    "        if \"inputs\" not in data or \"outputs\" not in data:\n",
    "            print(\"Error: 'inputs' or 'outputs' key not found in the JSON file.\")\n",
    "            return\n",
    "        \n",
    "        # 入力データと出力データをNumPy配列に変換\n",
    "        input_data = np.array(data[\"inputs\"])\n",
    "        output_data = np.array(data[\"outputs\"])\n",
    "    \n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: {input_path} not found.\")\n",
    "        return\n",
    "    except json.JSONDecodeError:\n",
    "        print(f\"Error: Failed to parse JSON file: {input_path}.\")\n",
    "        return\n",
    "    except Exception as e:\n",
    "        print(f\"Unexpected error while reading JSON file: {e}\")\n",
    "        return\n",
    "\n",
    "    # スケーラーをインスタンス化\n",
    "    input_scaler = MinMaxScaler()\n",
    "    output_scaler = MinMaxScaler()\n",
    "\n",
    "    try:\n",
    "        # 入力と出力を正規化\n",
    "        inputs_normalized = input_scaler.fit_transform(input_data)\n",
    "        outputs_normalized = output_scaler.fit_transform(output_data)\n",
    "\n",
    "        # 正規化データを辞書にまとめる\n",
    "        normalized_data = {\n",
    "            \"inputs\": inputs_normalized.tolist(),\n",
    "            \"outputs\": outputs_normalized.tolist()\n",
    "        }\n",
    "\n",
    "        # 正規化データをファイルに保存\n",
    "        with open(output_path, \"w\") as output_file:\n",
    "            json.dump(normalized_data, output_file)\n",
    "        \n",
    "        # スケーラーを保存するディレクトリを作成（存在しない場合）\n",
    "        os.makedirs(scaler_dir, exist_ok=True)\n",
    "        \n",
    "        # スケーラーを保存\n",
    "        input_scaler_path = os.path.join(scaler_dir, \"input_scaler.pkl\")\n",
    "        output_scaler_path = os.path.join(scaler_dir, \"output_scaler.pkl\")\n",
    "        with open(input_scaler_path, \"wb\") as input_scaler_file:\n",
    "            joblib.dump(input_scaler, input_scaler_file)\n",
    "        with open(output_scaler_path, \"wb\") as output_scaler_file:\n",
    "            joblib.dump(output_scaler, output_scaler_file)\n",
    "\n",
    "        print(\"Normalization and scaler saving completed successfully.\")\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error during normalization or saving process: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, json_path):\n",
    "        with open(json_path, 'r') as f:\n",
    "            data = json.load(f)\n",
    "        self.inputs = data[\"inputs\"]\n",
    "        self.outputs = data[\"outputs\"]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.inputs)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        x = torch.tensor(self.inputs[idx], dtype=torch.float32)\n",
    "        y = torch.tensor(self.outputs[idx], dtype=torch.float32)\n",
    "        return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dataloaders(json_path, batch_size=16, val_split=val_split, random_seed=random_seed):\n",
    "    dataset = CustomDataset(json_path)\n",
    "    \n",
    "    # 乱数シードを設定\n",
    "    torch.manual_seed(random_seed)\n",
    "    random.seed(random_seed)\n",
    "    \n",
    "    # 訓練データと検証データのサイズ計算\n",
    "    dataset_size = len(dataset)\n",
    "    val_size = int(dataset_size * val_split)\n",
    "    train_size = dataset_size - val_size\n",
    "    \n",
    "    # データセットを分割\n",
    "    train_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n",
    "    \n",
    "    # DataLoaderの作成\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "    \n",
    "    return train_loader, val_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# モデル定義\n",
    "class NetworkModel(nn.Module):\n",
    "    def __init__(self, input_dim=input_dim, output_dim=output_dim):\n",
    "        super(NetworkModel, self).__init__()\n",
    "        self.network = nn.Sequential(\n",
    "            nn.Linear(input_dim, 16),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(16, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(32, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(512, 1024),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(1024, 2048),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(2048, 1024),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(1024, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(32, 8),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(8, output_dim)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.network(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_and_prepare_directories():\n",
    "    # クリーンアップ対象のディレクトリを指定\n",
    "    directories_to_clean = [\n",
    "        \"../data/summary\",\n",
    "        \"../images/model_graph\",\n",
    "        \"../images/graph_training_results\",\n",
    "        \"../images/graph_val_results\",\n",
    "        \"../results\"\n",
    "    ]\n",
    "    \n",
    "    # 各ディレクトリ内の既存ファイルを削除\n",
    "    for dir_path in directories_to_clean:\n",
    "        if os.path.exists(dir_path):\n",
    "            shutil.rmtree(dir_path)  # ディレクトリとその中身を削除\n",
    "        os.makedirs(dir_path, exist_ok=True)  # 再作成\n",
    "\n",
    "    print(\"Directories cleaned and prepared.\")\n",
    "\n",
    "# トレーニング開始前にディレクトリをクリーンアップ\n",
    "clean_and_prepare_directories()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(trial, input_dim=input_dim, output_dim=output_dim, patience=patience, delta=delta, lr_min=lr_min, lr_max=lr_max, batch_size_min=batch_size_min, batch_size_max=batch_size_max, epochs_min=epochs_min, epochs_max=epochs_max):\n",
    "    # モデルの作成\n",
    "    input_dim = input_dim\n",
    "    output_dim = output_dim\n",
    "    model = NetworkModel()\n",
    "    \n",
    "        # トライアルごとに保存\n",
    "    trial_number = trial.number + 1  # トライアル番号\n",
    "    \n",
    "    # Early Stoppingのパラメータ\n",
    "    patience = patience  # 検証損失が改善しない許容エポック数\n",
    "    delta = delta   # 検証損失の改善として認める最小値\n",
    "    early_stop_counter = 0  # 改善しないエポック数をカウント\n",
    "    \n",
    "    # モデルサマリーの保存\n",
    "    os.makedirs(\"../data/summary\", exist_ok=True)\n",
    "    stdout_backup = sys.stdout  # 標準出力のバックアップ\n",
    "    sys.stdout = io.StringIO()  # 標準出力をキャプチャ\n",
    "    summary(model, (1, input_dim))  # モデルのサマリーを取得 (バッチサイズ1の例)\n",
    "    summary_str = sys.stdout.getvalue()  # サマリー内容を取得\n",
    "    sys.stdout = stdout_backup  # 標準出力を元に戻す\n",
    "    with open(f\"../data/summary/model_summary_trial_{trial_number}.txt\", \"w\") as f:\n",
    "        f.write(summary_str)\n",
    "    \n",
    "    # Optunaのトライアルからハイパーパラメータを取得\n",
    "    lr = trial.suggest_float(\"lr\", lr_min, lr_max)\n",
    "    batch_size = trial.suggest_int(\"batch_size\", batch_size_min, batch_size_max)\n",
    "    epochs = trial.suggest_int(\"epochs\", epochs_min, epochs_max)\n",
    "    optimizer_name = trial.suggest_categorical(\n",
    "    \"optimizer\", [\"Adam\", \"SGD\", \"RMSprop\", \"AdamW\", \"Adagrad\", \"Adadelta\", \"Adamax\"]\n",
    ")\n",
    "    loss_function_name = trial.suggest_categorical(\n",
    "    \"loss_function\", [\"MSELoss\", \"L1Loss\", \"SmoothL1Loss\", \"HuberLoss\", \"BCEWithLogitsLoss\", \"CrossEntropyLoss\"]\n",
    ")\n",
    "    \n",
    "    # Optimizer を選択\n",
    "    if optimizer_name == \"Adam\":\n",
    "        optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "    elif optimizer_name == \"SGD\":\n",
    "        momentum = trial.suggest_float(\"momentum\", 0.0, 0.9)  # SGD の場合のみ momentum を提案\n",
    "        optimizer = optim.SGD(model.parameters(), lr=lr, momentum=momentum)\n",
    "    elif optimizer_name == \"RMSprop\":\n",
    "        optimizer = optim.RMSprop(model.parameters(), lr=lr)\n",
    "    elif optimizer_name == \"AdamW\":\n",
    "        weight_decay = trial.suggest_float(\"weight_decay\", 1e-6, 1e-2)  # AdamW用のweight decay\n",
    "        optimizer = optim.AdamW(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "    elif optimizer_name == \"Adagrad\":\n",
    "        optimizer = optim.Adagrad(model.parameters(), lr=lr)\n",
    "    elif optimizer_name == \"Adadelta\":\n",
    "        optimizer = optim.Adadelta(model.parameters(), lr=lr)\n",
    "    elif optimizer_name == \"Adamax\":\n",
    "        optimizer = optim.Adamax(model.parameters(), lr=lr)\n",
    "            \n",
    "    # 損失関数を選択\n",
    "    if loss_function_name == \"MSELoss\":\n",
    "        criterion = nn.MSELoss()\n",
    "    elif loss_function_name == \"L1Loss\":\n",
    "        criterion = nn.L1Loss()\n",
    "    elif loss_function_name == \"SmoothL1Loss\":\n",
    "        beta = trial.suggest_float(\"beta\", 0.1, 1.0)  # SmoothL1Loss 用の beta を提案\n",
    "        criterion = nn.SmoothL1Loss(beta=beta)\n",
    "    elif loss_function_name == \"HuberLoss\":\n",
    "        delta = trial.suggest_float(\"delta\", 1.0, 10.0)  # HuberLoss用のdelta\n",
    "        criterion = nn.HuberLoss(delta=delta)\n",
    "    elif loss_function_name == \"BCEWithLogitsLoss\":\n",
    "        criterion = nn.BCEWithLogitsLoss()\n",
    "    elif loss_function_name == \"CrossEntropyLoss\":\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "    \n",
    "    # データローダーの取得\n",
    "    train_loader, val_loader = get_dataloaders('../data/normalized_data.json', batch_size=batch_size)\n",
    "    \n",
    "    train_losses = []  # 訓練損失を記録\n",
    "    val_losses = [] #検証損失の記録\n",
    "    val_accuracies = [] #バリデーション精度の記録\n",
    "    \n",
    "    best_loss = float(\"inf\")\n",
    "    best_model = None\n",
    "    \n",
    "    # トレーニングの進行状況をプログレスバーで表示\n",
    "    progress_bar = tqdm(\n",
    "        range(epochs),\n",
    "        desc=f\"Trial {trial_number} Progress\",  # トライアル番号付きの進行状況\n",
    "        unit=\"epoch\",\n",
    "        bar_format=\"{l_bar}{bar:40}| {n_fmt}/{total_fmt} epochs [{elapsed}<{remaining}] - {postfix}\"\n",
    "        )\n",
    "    \n",
    "    # エポックごとの訓練ループ\n",
    "    for epoch in progress_bar:\n",
    "        # 訓練フェーズ\n",
    "        model.train()  # モデルを訓練モードに設定\n",
    "        total_train_loss = 0  # 訓練損失の合計\n",
    "\n",
    "        for x, y in train_loader:\n",
    "            optimizer.zero_grad()  # 勾配をゼロにリセット\n",
    "            y_pred = model(x)  # モデルで予測\n",
    "            loss = criterion(y_pred, y)  # 損失を計算\n",
    "            loss.backward()  # 逆伝播\n",
    "            optimizer.step()  # パラメータを更新\n",
    "            total_train_loss += loss.item()  # 訓練損失を累積\n",
    "\n",
    "        avg_train_loss = total_train_loss / len(train_loader)  # 平均訓練損失\n",
    "        train_losses.append(avg_train_loss)  # 訓練損失を記録\n",
    "\n",
    "        # 検証フェーズ\n",
    "        model.eval()  # モデルを評価モードに設定\n",
    "        total_val_loss = 0  # 検証損失の合計\n",
    "        correct_predictions = 0  # 正しい予測の数\n",
    "        total_samples = 0  # サンプル数\n",
    "\n",
    "        with torch.no_grad():  # 検証時は勾配計算を行わない\n",
    "            for x, y in val_loader:\n",
    "                y_pred = model(x)  # モデルで予測\n",
    "                loss = criterion(y_pred, y)  # 損失を計算\n",
    "                total_val_loss += loss.item()  # 検証損失を累積\n",
    "\n",
    "                # 精度を計算\n",
    "                _, predicted = torch.max(y_pred, 1)  # 予測結果\n",
    "                _, labels = torch.max(y, 1)  # 正解ラベル\n",
    "                correct_predictions += (predicted == labels).sum().item()  # 正しい予測の数をカウント\n",
    "                total_samples += labels.size(0)  # サンプル数をカウント\n",
    "\n",
    "        avg_val_loss = total_val_loss / len(val_loader)  # 平均検証損失\n",
    "        val_losses.append(avg_val_loss)  # 検証損失を記録\n",
    "\n",
    "        accuracy = 100 * correct_predictions / total_samples  # 精度の計算\n",
    "        val_accuracies.append(accuracy)  # 精度を記録\n",
    "\n",
    "        # 最良モデルを保存\n",
    "        if avg_val_loss < best_loss - delta:\n",
    "            best_loss = avg_val_loss\n",
    "            best_model = model.state_dict()\n",
    "            early_stop_counter = 0  # 改善があればカウンターをリセット\n",
    "\n",
    "        else:\n",
    "            early_stop_counter += 1  # 改善しない場合にカウンターを増加\n",
    "            \n",
    "         # プログレスバーに詳細な情報を表示\n",
    "        progress_bar.set_postfix({\n",
    "            \"TrainLoss\": f\"{avg_train_loss:.4f}\",\n",
    "            \"ValLoss\": f\"{avg_val_loss:.4f}\",\n",
    "            \"ValAcc\": f\"{accuracy:.2f}%\",\n",
    "            \"ES\": f\"{early_stop_counter}/{patience}\",  # Early stoppingのカウントと許容値\n",
    "            \"LR\": f\"{lr:.2e}\",  # 学習率を科学記号形式で表示\n",
    "            \"Batch\": batch_size,\n",
    "            \"Opt\": optimizer.__class__.__name__,\n",
    "            \"LossFn\": loss_function_name\n",
    "        })\n",
    "\n",
    "        # Optunaの進捗を報告\n",
    "        trial.report(avg_val_loss, epoch)\n",
    "        \n",
    "        # Early Stoppingの判定\n",
    "        if early_stop_counter >= patience:\n",
    "            print(f\"Early stopping at epoch {epoch + 1}\")\n",
    "            break\n",
    "\n",
    "        # プルーニングが必要ならば中断\n",
    "        \"\"\"if trial.should_prune():\n",
    "            raise optuna.exceptions.TrialPruned()\"\"\"\n",
    "        \n",
    "        \n",
    "        \n",
    "    # トライアルごとに学習曲線とバリデーション精度をプロットして保存\n",
    "    os.makedirs('../images/graph_training_results', exist_ok=True)  # images/graphディレクトリがない場合は作成\n",
    "    os.makedirs('../images/graph_val_results', exist_ok=True)\n",
    "\n",
    "    # 学習曲線をプロット\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(train_losses, label=\"Train Loss\")\n",
    "    plt.plot(val_losses, label=\"Validation Loss\")\n",
    "    plt.xlabel(\"Epochs\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.title(f\"Training and Validation Loss (Trial {trial_number})\")\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.savefig(f'../images/graph_training_results/training_validation_loss_trial_{trial_number}.png')  # トライアルごとに保存\n",
    "    plt.close()\n",
    "\n",
    "    # バリデーション精度のグラフをプロット\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(val_accuracies, label=\"Validation Accuracy\")\n",
    "    plt.xlabel(\"Epochs\")\n",
    "    plt.ylabel(\"Accuracy (%)\")\n",
    "    plt.title(f\"Validation Accuracy (Trial {trial_number})\")\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.savefig(f'../images/graph_val_results/validation_accuracy_trial_{trial_number}.png')  # トライアルごとに保存\n",
    "    plt.close()\n",
    "\n",
    "\n",
    "    if not os.path.exists('../results'):\n",
    "        os.makedirs('../results')\n",
    "        \n",
    "    # トライアルごとに最良モデルを保存\n",
    "    torch.save(best_model, f'../results/best_model_trial_{trial_number}.pth')  # トライアルごとに保存\n",
    "\n",
    "    # 最良モデル（全トライアル）を保存\n",
    "    if best_model is not None:\n",
    "        # 最初のトライアルであれば最良モデルを保存、次のトライアルで更新\n",
    "        if trial.number == 0 or avg_val_loss < best_loss:\n",
    "            torch.save(best_model, '../results/best_model.pth')\n",
    "\n",
    "    return best_loss # 最良の検証損失を返す"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def perform_bayesian_optimization(n_trials=n_trials):\n",
    "    \"\"\"Optunaによるベイズ最適化の実行\"\"\"\n",
    "    study = optuna.create_study(direction='minimize')\n",
    "    n_trials = n_trials\n",
    "    progress_bar = tqdm(\n",
    "        total=n_trials,\n",
    "        desc=f\"{Fore.MAGENTA}Bayesian Optimization Progress{Style.RESET_ALL}\",\n",
    "        unit=\"trial\",\n",
    "        bar_format=\"{l_bar}{bar:30}| {n_fmt}/{total_fmt} [{elapsed}<{remaining}] - {desc}\"\n",
    "    )\n",
    "\n",
    "    def callback(study, trial):\n",
    "        # プログレスバーの更新\n",
    "        progress_bar.set_description(\n",
    "            f\"{Fore.BLUE}Trial {trial.number+1}{Style.RESET_ALL} | Best Loss: {Fore.RED}{study.best_value:.6f}{Style.RESET_ALL}\"\n",
    "        )\n",
    "        progress_bar.update(1)\n",
    "\n",
    "        # ベストハイパーパラメータと損失をリアルタイム表示\n",
    "        print(f\"\\n{Fore.YELLOW}Current Best Hyperparameters:{Style.RESET_ALL} {study.best_params}\")\n",
    "        print(f\"{Fore.YELLOW}Current Best Loss:{Style.RESET_ALL} {study.best_value:.6f}\")\n",
    "\n",
    "    # ベイズ最適化の実行\n",
    "    study.optimize(lambda trial: train_model(trial), n_trials=n_trials, callbacks=[callback])\n",
    "\n",
    "    # プログレスバーを閉じる\n",
    "    progress_bar.close()\n",
    "\n",
    "    # 最終結果の表示\n",
    "    best_trial = study.best_trial\n",
    "    print(f\"{Fore.GREEN}Best Trial Number:{Style.RESET_ALL} {best_trial.number+1}\")\n",
    "    print(f\"{Fore.GREEN}Best Hyperparameters:{Style.RESET_ALL} {best_trial.params}\")\n",
    "    print(f\"{Fore.GREEN}Best Loss:{Style.RESET_ALL} {best_trial.value}\")\n",
    "\n",
    "    return best_trial.params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# JSONデータの正規化\n",
    "normalize_json_data('../data/data.json', '../data/normalized_data.json')\n",
    "\n",
    "# Optunaによるベイズ最適化の実行\n",
    "best_params = perform_bayesian_optimization()\n",
    "print(\"Optimization completed. Best parameters:\", best_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 推論用関数 (モデルとスケーラーをグローバルに読み込む)\n",
    "def load_model_and_scalers(model_path='../results/best_model.pth'):\n",
    "    \"\"\"モデルとスケーラーを読み込む\"\"\"\n",
    "    model = NetworkModel()\n",
    "    model.load_state_dict(torch.load(model_path))\n",
    "    model.eval()\n",
    "    \n",
    "    input_scaler = joblib.load('../data/input_scaler.pkl')\n",
    "    output_scaler = joblib.load('../data/output_scaler.pkl')\n",
    "    \n",
    "    return model, input_scaler, output_scaler\n",
    "\n",
    "model, input_scaler, output_scaler = load_model_and_scalers()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " # 推論用関数\n",
    "def generate_output(input_data, model=model, input_scaler=input_scaler, output_scaler=output_scaler):\n",
    "    input_data_scaled = input_scaler.transform([input_data])\n",
    "    input_tensor = torch.tensor(input_data_scaled, dtype=torch.float32)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        output_scaled = model(input_tensor)\n",
    "        output = output_scaler.inverse_transform(output_scaled.numpy())\n",
    "\n",
    "    return output[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 推論\n",
    "test_input = [test_x, test_y, test_z]\n",
    "output = generate_output(test_input)\n",
    "print(f\"Input data: {test_input}\")\n",
    "print(f\"Generated Output: {output}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
